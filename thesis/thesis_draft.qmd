---
format:
  pdf:
    documentclass: report
    papersize: letter
    fontsize: 12pt
    mainfont: Times New Roman
    geometry:
      - left=1.5in
      - right=1in
      - top=1in
      - bottom=1in
    include-before-body:
      - title-page.tex
      - copyright-page.tex
      - committee-page.tex
      - abstract.tex
      - acknowledgments.tex
      - table-of-contents.tex
    number-sections: true
    citation-style: apa
    bibliography: freq_itemsets_citations.bib
    csl: apa-6th-edition.csl
    include-in-header: formating.tex
eval: false
---

```{r}
#| include: false
#| message: false
#| eval: true

library(tidyverse)
library(tidyclust)
library(arules)
library(yardstick)
library(dials)
library(kableExtra)
library(palmerpenguins)
library(recipes)
library(workflows)



data(Groceries)
groceries <- as.data.frame(as(Groceries, "matrix")) |>
  mutate(across(everything(), ~.*1))
set.seed(130)
na_result <- tidyclust:::random_na_with_truth(groceries[1:5,], na_prob = 0.3)
new_groceries <- na_result$na_data
truth <- na_result$truth
sample_indices <- sample(nrow(penguins), size = floor(0.10 * nrow(penguins)))
penguins_new <- penguins[sample_indices, ]
```

# Introduction

Frequent itemset mining (FIM) has been a growing area of research in data mining since the early 1990s. The work of Agrawal, Imieliński, and Swami [-@agrawal_mining_1993] introduced the Apriori algorithm, which established the foundation for identifying co-occurring items in transactional datasets. Shortly after, algorithms such as FP-growth’s FP-tree structure [@han_mining_2000] and Elcat’s vertical tid-list approach [@zaki_scalable_2000] significantly improved the performance and scalability of FIM techniques. These techniques have since been applied to a wide range of applications, including market basket analysis, web personalization [@mobasher_effective_2001], and associative classification [@liu_integrating_1998].

Although FIM has become more efficient, practical implementations are often confined to specialized software packages. For example, the R programming environment offers itemset mining through the `arules` package [@hahsler_arules_2011], while Python users rely on `mlxtend` [@raschka_mlxtend_2018]. However, these implementations typically operate as standalone applications, lacking integration within modern data science workflows. This absence of native integration with frameworks like `tidymodels` [@kuhn_tidymodels_2020] or `scikit-learn` [@buitinck_api_2013] creates a need for analysts to develop custom solutions.

Another limitation lies in the uses of FIM results. Frequent itemsets are commonly treated as either final outputs, or inputs for generating association rules. However, research by Cheng et al. [-@cheng_discriminative_2007] demonstrated that itemsets can also be used in classification tasks, while other studies have explored how itemsets can be used in clustering tasks [@mobasher_effective_2001; @wickramaratna_predicting_2009]. Although these approaches lack a standardized method, the use of FIM in these methods indicates a potential for integration with clustering workflows.

This thesis addresses these limitations by introducing FIM to `tidyclust` [@hvitfeldt_tidyclust_2022], a package designed for unsupervised learning algorithms within the `tidymodel` framework. By adapting the Apriori and Eclat algorithms, this work allows itemset mining to be used within unsupervised workflows. Furthermore, it establishes a standard methodology for predicting missing items and lays a foundation for integrating future data mining and clustering techniques into `tidyclust`. The source code for the work in this thesis can be found at https://github.com/Wander03/tidyclust.

# Background

## Frequent Itemset Mining: Concepts and Algorithms

Frequent itemset mining (FIM) methods were developed to identify the elements within a transactional dataset that often occur together. Initially used in market basket research [@agrawal_mining_1993], FIM techniques help discover items commonly purchased together. 

```{r}
#| eval: true
#| echo: false
#| tbl-cap: "Example Grocery Transactional Dataset"
data.frame(
  'milk' = c(1, 0, 1, 1, 1),
  'eggs' = c(0, 1, 0, 0, 0),
  'beer' = c(0, 1, 1, 1, 0),
  'diapers' = c(0, 1, 1, 1, 1),
  'bread' = c(1, 1, 0, 1, 1)
) |> kable(align = 'r')
```

For instance, a pattern could be that milk and bread are frequently bought together in the same transaction. This type of information can be used by the shop owner to place these items apart from each other, causing customers to browse more of the store. Although seemingly simple, the number of itemsets expands rapidly in larger datasets, requiring more efficient algorithms. One key development is the Apriori principle (or downward closure property), which states that if an itemset is frequent, then so are all of its subsets. This principle significantly reduces the search space by systematically pruning candidate itemsets that fall below a minimum support threshold. The `tidyclust` extensions in this thesis use Apriori and Eclat, which both operate on this principle.

### Definitions

This section further defines and formalizes key terms used in frequent itemset mining.

Let $I = \{i_1, i_2, \dots, i_m\}$ be a set of items and $D = \{t_1, t_2, \dots, t_n\}$ a transactional dataset where each transaction $t_j \subseteq I$. A non-empty subset $X = \{i_1, i_2, ..., i_k\} \subseteq I$ is an itemset, or k-itemset where $k$ is the number of items.

The support of an itemset $X$ is the proportion of transactions containing $X$:

$$\text{support}(X) = \frac{|{t \in D : X ⊆ t}|}{|D|}$$

An itemset is considered frequent if its support is greater than or equal to a given minimum support threshold $\sigma$, where $0 \leq \sigma \leq 1$. The goal of FIM is to find the set of frequent itemsets corresponding to the users minimum support.

### Apriori: Breadth-First Search

The Apriori algorithm [@agrawal_mining_1993] implements a breadth-first search to identify frequent itemsets, leveraging the downward closure property. The method operates as follows:

1.  Initialization ($k = 1$): Scan the transactional dataset $D$ to compute the support of all 1-itemsets $X$. Keep only those with support($X$) $\geq \sigma$, forming the set of frequent 1-itemsets $L_1$.

2.  Candidate Generation ($k > 1$): Generate candidate k-itemsets $C_k$ by joining pairs of frequent ($k$ - 1)-itemsets from $L_{k-1}$ that share the first $k$ - 2 items:
$$C_k = \{X \cup Y : X, Y \in L_{k - 1}, |X \cap Y| = k - 2\}$$

3.  Pruning: Eliminate any candidate $X \in C_k$ where ($k$ - 1)-subset of $X$ is not in $L_{k - 1}$ (downward closure property)

4.  Support Counting: Scan the full dataset $D$ to compute the support($X$) for all $X \in C_k$.

5.  Iteration: Repeat steps 2-4 until no new frequent itemsets are found ($L_k = \emptyset$). The set of all frequent itemsets is 
$\bigcup_k L_k$.

### Eclat: Vertical Approach

The Equivalence Class Transformation (Eclat) algorithm [@zaki_scalable_2000] is an alternative to Apriori that uses a depth-first search strategy and vertical data representation. Instead of repeatedly scanning the dataset, Eclat represents transactions using tid-lists (transaction ID lists), which map each item or itemset to the IDs of transactions in which it appears. The method operates as follows:

1.  Vertical Representation: Transform the transaction dataset $D$ into a vertical format, where each item $i$ maintains its tid-list:
$$
T(i) = {t \in D : i \in t}
$$

Initialize the set of frequent 1-items:
$$
L_1 = \{\{i\} : |T(i)| \geq \sigma \times |D|\}
$$

2.  Depth-First Search: For each frequent itemset $X \in L_k$:

<!-- -->

a.  Candidate Generation: Extend $X$ with items $i > max(X)$ (lexicographic order to avoid redundant candidate generation) to form ($k$ + 1)-itemset candidates $X' = X \cup \{i\}$.

b.  Tid-list Intersection: Compute the tid-list of $X’$ with set intersection: $$T(X’) = \bigcap_{i \in X’} T(i)$$

c.  Support Verification: Keep $X’$ if: 
$$
\frac{|T(X’)|}{|D|} \geq \sigma
$$ 
Add $X’$ to $L_{k + 1}$

<!-- -->

3.  Termination: Repeat step 2 until no new frequent itemsets are found ($L_k = \emptyset$). The set of all frequent itemsets is 
$\bigcup_k L_k$.

## The tidyclust Package

The `tidyclust` package [@hvitfeldt_tidyclust_2022] extends the `tidymodels` [@kuhn_tidymodels_2020] framework to unsupervised tasks. These packages were built following the principles of the `tidyverse` [@wickham_welcome_2019], with the goal of establishing a consistent and reproducible workflow. Modeled closely on `parsnip` [@kuhn_parsnip_2024], `tidyclust` enables users to specify, fit, and predict with unsupervised learning models using a standardized syntax.

### Model Specification

The `tidyclust` workflow mirrors established practices in supervised modeling, beginning with model specification and selection. For example, a k-means model with three clusters is specified as:

```{r}
#| prompt: true
#| eval: true
kmeans_spec <- k_means(num_clusters = 3) |>
  set_engine("stats")
```

Here, the function, `k_means` is the name of the model, and the parameter(s) (e.g., `num_clusters`) are the required and optional inputs for the model. In this case, `num_clusters = 3` defines a k-means model with three clusters. The `set_engine()` function selects the package implementation the model is being run on, here this is the `stats` package. The resulting object is called a `fi_spec()` object.

### Data Preprocessing

Data preparation is taken care of by the `recipes` package [@kuhn_recipes_2024]. This package support a range of data transformations, including: feature scaling (e.g., normalization, standardization), categorical data encoding (e.g., dummy variables), data cleaning (e.g., missing value removal/imputation), and dimensionality  reduction (e.g., PCA, feature filtering). The package follows a declarative syntax, improving both readability by separating the data preparation code from the modeling code and reproducibility by ensuring consistent transformations across training and testing data.

The following example uses the `palmerPenguins` dataset to create a recipe for k-means clustering:

```{r}
#| prompt: true
#| eval: true
recipe <- recipe(
  ~ bill_length_mm + body_mass_g + sex, 
  data = penguins
  ) |>
  step_naomit(all_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())
```

The recipe specifies predictors for the model and then removes any rows with missing values. It also normalizes numeric predictors and creates dummy variables for categorical predictors. When specifying a formula for clustering models, no response variable is used since clustering methods are unsupervised. Because these transformations are applied within the recipe, they do not alter the dataset itself. Furthermore, the same recipe can be reused across different models built into the `tidyclust` or `tidymodels` framework.

### Workflow Integration

The `workflows` package [@vaughan_workflows_2024] combines the model definition and feature engineering into a single object, improving reproducibility, reducing risk of human error, and standardizing the modeling pipeline. This modular design allows users to modify preprocessing steps without changing model specifications and to reuse workflow elements across projects, ensuring consistency. For instance, an implementation using the previous examples is shown below:

```{r}
#| prompt: true
#| eval: true
kmeans_workflow <- workflow() |>  
  add_recipe(recipe) |>  
  add_model(kmeans_spec)
```

### Model Fitting

The final phase of the `tidyclust` workflow includes model fitting and prediction. The `fit()` function trains the model on the data, and can directly use the `fi_spec()` object or a workflow as shown below.

```{r}
#| prompt: true
#| eval: true
kmeans_fit <- fit(kmeans_workflow, data = penguins)
```

This fitted object is called a `cluster_fit` object, and contains the trained model with any metadata required for later operations.

### Model Prediction

The `predict()` function uses the fitted object and new observations as inputs, producing results in a standardized format:

```{r}
#| prompt: true
#| eval: true
kmeans_predict <- predict(kmeans_fit, new_data = penguins_new)
```

The behavior of `predict()` varies based on the type of fitted model. For example, the k-means model from before predicts the cluster assignment for each new observation in the input data by identifying the nearest cluster center from the fitted model.

### Evaluation Metrics

Since unsupervised learning algorithms have no target or outcome variables, there is no inherent metric for measuring the success of predictions. However, some metrics are implemented in `tidyclust` to measure the quality of the created clusters. For partition clustering like k-means, these metrics are sum of squared error (SSE) and silhouette. Here we omit the details of the metrics and show only the structure of the code to cacluclate them from a fitted cluster model:

```{r}
#| prompt: true
kmeans_fit |> 
  sse_within_total()

kmeans_fit |> 
  sse_total()

kmeans_fit |> 
  sse_ratio()

kmeans_fit |> 
  silhouette_avg(penguins)
```

While specific metrics vary across `tidyclust` models, they can all be calculated in the format above.

### Hyperparameter Tuning

The `tidyclust` framework provides the ability to use existing resampling and hyperparameter tuning methods on unsupervised models. Grid search is implemented using the `tune` package [@kuhn_tune_2024], and can be paired with resampling methods like cross-validation to evaluate a model. Using the k-means example above, cross-validation is set up with 5 folds (`v = 5`).

```{r}
#| prompt: true
penguins_cv <- vfold_cv(penguins, v = 5)
```

In this model, since we are tuning the number of clusters `num_clusters` is changed from `3` to `tune()`, and a workflow is created.

```{r}
#| prompt: true
tune_spec <- k_means(num_clusters = tune()) |>
  set_engine("stats")

tune_workflow <- workflow() |>  
  add_recipe(recipe) |>  
  add_model(tune_spec)
```

To establish the range of possible values for `num_clusters`, we use `grid_regular()` to create a grid of values to iterate over.

```{r}
#| prompt: true
clust_num_grid <- grid_regular(
  num_clusters(3, 10),
  levels = 5
)
```

Finally, grid search is carried out in `tune_cluster()`, with the metrics for each cross-validation split and grid point specified within `tune_cluster()`.

```{r}
#| prompt: true
rf_grid_search <-
  tune_cluster(
    tune_workflow,
    resamples = penguins_cv,
    grid = clust_num_grid,
    metrics = cluster_metric_set(
      sse_within_total, 
      sse_total, 
      sse_ratio
      )
  )
```

The remaining chapters of this thesis will be dedicated to the examination of the design and implementation choices made while implementing FIM in `tidyclust.` Chapter 3 details each step of the workflow and Chapter 4 addresses current limitations and proposed enhancements.

# Frequent Itemset Mining with tidyclust

## Model Specification

The model specification for frequent itemset mining follows the same structure as other models in `tidyclust`, creating a `fi_spec()` object:

```{r}
#| prompt: true
#| eval: true
#| output: false
fi_spec <- freq_itemsets(
  min_support = 0.05, 
  mining_method = "eclat"
  ) |>
  set_engine("arules")
```

The only available engine is `arules`. There are two arguments: `min_support` specifies the user's minimum support threshold (between 0 and 1), and `mining_method` (optional) selects the algorithm (`apriori` or `eclat`). The default `mining_method` is `eclat` because it is faster in most practical cases [@singla_comprehensive_2023]. A default value for `min_support` is not supplied since this value greatly varies depending on the characteristics of the data. Recipes and workflows are not impacted by any additions made to `tidyclust`, remaining compatible with `freq_itemsets`.

## Cluster Assignment Methodology

While fitting a `freq_itemsets` model uses the same syntax, the output required several design choices:

```{r}
#| prompt: true
#| eval: true
#| output: false
fi_fit <- fi_spec |>
  fit(~ .,
    data = groceries
  )
```

A key strength of the `tidyclust` framework is its standardized output format across functions. Specifically, fitted models primarily output cluster assignments in a `tibble` with a single column named `.cluster`.

```{r}
#| prompt: true
#| eval: true
fi_fit |> 
  extract_cluster_assignment() |>
  head(5)
```

However, unlike other unsupervised learning algorithms (e.g., k-means) that explicitly partition observations (rows) into distinct clusters, FIM identifies co-occurring items (columns) without inherent cluster labels. To address this, the following implementation introduces a clustering assignment strategy that assigns items to clusters based on itemset membership and support values:

For each items in the dataset:

1. Relevant Itemset Identification: Finds all itemsets containing the item.

2. Best Itemset Selection: Prioritizes itemsets by:

    + Size: Larger itemsets over smaller ones (encouraging broader clusters).

    + Support: Higher-support itemsets break ties (favoring more frequent patterns).

3. Cluster Propagation: Assigns all items in the selected itemsets to the same cluster. 

If an item has already been assigned a cluster, the algorithm compares the current "best" itemset with the itemset under consideration. It re-prioritizes based on the criteria above and reassigns all relevant items to a new cluster if the latter is superior. To remove any effect of item ordering, this strategy repeats until no items are reassigned to a new cluster (convergence).

Items that appear in no frequent itemsets are labeled as outliers (Cluster_0_X) while items within frequent itemsets are assigned sequential cluster IDs (Cluster_1, Cluster_2, etc.).

This prioritization strategy aligns with findings by Cheng et al. [-@cheng_discriminative_2007], who analyzed the relationship between pattern frequency and discriminative power, demonstrating that frequent patterns with higher support and larger sizes exhibit greater predictive utility in classification tasks. These findings are used as the reasoning for prioritizing larger itemsets with higher supports. 

## Prediction Methods

Similar to the `fit()` method, while the syntax for predicting using a fitted `freq_itemsets` model is the same, the input and methodology of the `predict()` function differ from other `tidyclust` models:

```{r}
#| prompt: true
#| eval: true
fi_pred <- fi_fit |>
  predict(new_data = new_groceries, type = "raw")
```

When determining how to predict using frequent itemsets, the goal was to develop a method with practical use in real-world applications. This led to the implemention of a recommender system, where the new data contains partial information about a transaction, and the model predicts other items that are likely to be in the transaction. For example, suppose a customer in a grocery store has milk and bread in their cart. This system would predict which other items they are likely to add based on historical purchasing patterns where milk and bread appeared together.

The recommender system implementation draws inspiration from traditional association rule mining techniques like CARs [@liu_integrating_1998] and usage-based recommendations [@mobasher_effective_2001]. The method predicts item-level probabilities using the confidence from frequent itemsets, retains interpretability from a rule-based reasoning approach, and handles sparse data through a global support fallback. Specifically, for each transaction in the new data:

1. Missing Item Identification: Finds all missing items in the transaction.

For each missing item:

1. Relevant Itemset Identification: Finds all itemsets containing the item.

2. Context Filtering: Retain itemsets that include at least one observed item from the transaction.

3. Confidence Aggregation: For each retained itemset, compute the confidence of the association where
$$
\text{confidence} = \frac{\text{support(itemset)}}{\text{support(observed items in itemset)}}
$$

4. Prediction Generation: Average the confidences across all retained itemsets.

5. Fallback Handling: If there are no relevant/retained itemsets, use the item's global support (frequency in training data) as the prediction. If the item does not appear in any frequent itemsets, then this global support will be 0.

Aside from the `new_data` argument, `predict()` has the optional `type` argument which takes either 'raw' or 'cluster'. The raw option returns the raw prediction probabilities, while the default option cluster applies a threshold (0.5) to convert probabilities to binary predictions. Both options return the predictions as a `tibble` with the sole column `.pred_cluster`. The output of predict from the k-means model in Chapter 2 is:

```{r}
#| prompt: true
#| eval: true
kmeans_predict |>
  head(5)
```

While this `tibble` format is standardized across `tidyclust` predict outputs, it does not provide the information necessary for FIM predict output since FIM prediction on the items (columns) and not the observations (rows). To keep this standardized formatting while providing the necessary information, each row of the output `tibble` represents a transaction from the new data and holds a dataframe rather than a string or integer. 

```{r}
#| prompt: true
#| eval: true
fi_pred |>
  head(5)
```

This dataframe has three columns: `item`, `.obs_item`, and `.pred_item`. The `item` column represents each item (column) from the transaction, and `.obs_item` is `1` or `0` for any observed items and `NA` for any missing items. The `.pred_item` column is the opposite of the `.obs_item` column, where any observed items are `NA` and any missing items are either the raw or binary predicted values. 

```{r}
#| prompt: true
#| eval: true
#| tbl-cap: "Dataframe within .pred_cluster"
fi_pred$.pred_cluster[[1]] |>
  head(5) |>
  kable(align = 'r', digits = 4)
```

This structure allows the user to easily identify what values are observed or predicted in each transaction while still containing all the information necessary to recreate the original transaction dataset. To assist the user with reconstruction, we provide a new function `extract_predictions()` which takes the `tibble` output and reformat it to a single data frame.

```{r}
#| prompt: true
#| eval: true
#| tbl-cap: "extract_predictions() Output Dataframe"
fi_pred |>
  extract_predictions() |>
  head(c(5, 6)) |>
  kable(align = 'r', digits = 4)
```

## Evaluation Metrics

While support values traditionally assess frequent itemset quality, they do not guarantee predictive performance. Since the `predict()` methodology resembles a recommender system, the results should be evaluated using similar metrics. Common metrics such as root mean squared error (RMSE), accuracy, precision, and recall are implemented in the `yardstick` package [@kuhn_yardstick_2024]. The metrics require a list of predicted values and a list of the true values. To prepare the `predict()` output for metric calculation, we provide a new function `augment_itemset_predict()`.

```{r}
#| prompt: true
fi_fit |>
  predict(new_data = new_groceries, type = 'raw') |>
  augment_itemset_predict(truth) |>
  rmse(truth, preds)

fi_fit |>
  predict(new_data = new_groceries, type = 'cluster') |>
  augment_itemset_predict(truth) |>
  mutate(
    truth = factor(truth, levels = c(0, 1)), 
    preds = factor(preds, levels = c(0, 1))
    ) |>
  precision(truth, preds)
```

When using RMSE, the raw `predict()` output should be used, while accuracy, precision, and recall will use the cluster output or user thresholded `raw` output. To use the `precision()` function, the mutate step encodes the true and predicted values as factors, which are the required input types. Caution should be used when looking at accuracy, precision, and recall since they can be misleading for imbalanced datasets (where items are infrequently purchased). In such cases, F1-Score offers a balance between precision and recall, and precision-recall (PR) curves [@saito_precision-recall_2015] aid in determining the best threshold value.

```{r}
#| prompt: true
#| eval: true
#| fig-cap: "Precision-Recall Curve of Example Grocery Data"
fi_fit |>
  predict(new_data = new_groceries, type = 'raw') |>
  augment_itemset_predict(truth) |>
  mutate(truth = factor(truth, levels = c(0, 1))) |>
  pr_curve(truth, preds) |>
  autoplot()
```

Each point on the PR curve represents the precision and recall of the model at a specific threshold. By varying this threshold, we get different precision and recall values, creating the curve. The ideal curve is close to the top-right corner, indicating high precision and high recall. This curve shows that the model can achieve high precision and recall across a wide range of thresholds.

## Hyperparameter Tuning

The sole parameter capable of being tuned in a FIM model is `min_support`. As noted in Chapter 2.2.1 (Model Specification), there is no default `min_support` value since this value varies depending on the characteristics of the dataset. Therefore, selecting the correct value is imperative for finding useful frequent itemsets. The default grid for `min_support` is from 0.1 to 0.5. The lower bound of 0.1 was chosen to avoid reporting too many frequent itemsets, even for smaller datasets, while the upper bound of 0.5 was selected since it ensures that at each frequent itemset has a support of at least 50%.

```{r}
#| prompt: true
#| eval: true
grid_regular(
  min_support(), 
  levels = 10
  )
```

Usually the above object is paired with `tune_cluster` as seen in the previous k-means example, however cross-validation is not currently implemented for FIM. Suggestions for improved tuning and implementing cross validation will be discussed in chapters 4.3 and 4.5.

# Limitations and Future Directions

While this implementation successfully integrates frequent itemset mining into the `tidyclust` framework, there are several limitations. This section discusses these constraints and proposes potential enhancements to improve the methodology.

## Cluster Assignment Methodology

The current clustering method prioritizes itemsets based on size and support, assigning items to clusters by identifying the largest and most frequent patterns. However, relying on size and support alone may not always align with domain-specific objectives. For example, smaller but more distinct itemsets may be more informative in applications like fraud detection or medical diagnosis. Therefore, alternative metrics for selecting the "best" itemset could improve cluster quality in these situation.

## Prediction Methodology

The prediction implementation currently averages confidence scores across all matching itemsets to determine the predicted probability of a missing item. Additional methods may improve predictive performance, such as using the maximum confidence [@mobasher_effective_2001] or a weighted average by itemset size or support. These methods could be added as an optional parameter for the user in the predict function: 

```{r}
#| prompt: true
predict(
  fi_fit,
  new_data = new_groceries,
  confidence_agg = "max"
)
```

Furthermore, these methods do not incorporate the cluster assignments created when fitting the model. By weighting confidences higher for itemsets where multiple observed items share cluster membership with the predicted item, the predictive performance could be improved.

## Hyperparameter Tuning

The current implementation requires manual specification of `min_support` ranges for tuning. This presents challenges for users unfamiliar with their datasets characteristics or new to frequent itemset mining. The `tune` package used in `tidyclust` relies on the `dials` package [@kuhn_dials_2024], which offers `finalize` functions. These functions take the dataset as a parameter and modify the unknown parts of ranges based on the dataset. Building on Dahbi et al.'s [-@dahbi_finding_2021] statistically-grounded approach, I propose a two-step approach:

1. Calculate the mean support of all 1-itemsets:
$$
\mu = \frac{1}{n}\sum^n_{i=1}{\text{support}(\{\text{item}_i\})}
$$
The median or mode could also be used here.

2. Create a confidence-interval-like range around $\mu$ using the standard deviation $\sigma$:
$$
[\mu - \frac{\sigma}{2}, \mu + \sigma]
$$
clipping the bounds at $[0, 1]$ to ensure valid support values. An asymmetric range was selected since transactional datasets tend to have a right skewed support distribution.

## Evaluation Metrics

While the current implementation works with standard metrics through `augment_itemset_predict()`, specialized metrics could provide better insight into prediction performance. The functions output preserves both transaction-level and item-level identifiers, allowing for two different types of analysis:

1. Item-level metrics: measures how accurately specific items are predicted across all transactions.

2. Transaction-level metrics: evaluates complete basket prediction quality.

These metrics could be used to detect consistently poor predictions for niche items and pinpoint whether errors are from specific items or transaction patterns.

## Cross-Validation

The primary objective of cross-validation in frequent itemset mining is to determine the optimal `min_support` threshold that maximizes the model's predictive performance while maintaining generalizability. However, traditional k-fold CV randomly splits transactions, which would potentially break item co-occurrence patterns essential for FIM. Standard k-fold CV randomly partitions the dataset into k subsets (folds), using k-1 folds for training and the remaining fold for validation. This procedure assumes independence between training and validation data, but in FIM this breaks down for two reasons:

1. Pattern Fragmentation: Randomly splitting transactions may divide itemset occurrences across multiple folds, artificially reducing their observed support leading to underestimation of their importance.. 

2. Dependence Violation: Since frequent itemsets are derived from the entire dataset, any validation set inherently contains information from the training folds through shared itemsets. This violates the independence assumption fundamental to CV's statistical validity.

To address these issues, I propose item-stratified CV. This approach constructs folds that preserve the overall frequency distribution of items, ensuring that each fold is representative in terms of both item frequencies and co-occurrence patterns. By stratifying on items rather than transactions, we maintain the structural integrity of the itemsets and prevent support distortion across folds. For datasets with temporal components, such as purchases over time, using earlier time periods for training and later periods for validation would reflect real-world deployment. In both approaches, prediction is evaluated by masking a random subset (e.g., 20%–30%) of items within each transaction in the validation set. The model then predicts the masked items based on the observed partial basket. 

# Conclusion

This work provides new functionality in the `tidyclust` package by implementing a cluster specification for frequent itemset mining. Substantial modifications to existing `tidyclust` methods such as `fit()` and `predict()` were created to handle the unique output of the Apriori and Eclat algorithms. Furthermore, several new functions were built to support this implementation. These include `extract_predictions()` which reformats the complex prediction output to a user-friendly format, and `augment_itemset_predict()` that prepares prediction output for evaluation using standard metric functions.

## COMPUTATIONAL DETAILS

The results in this paper were obtained using `R` 4.4.1 with the
`tidyclust` 0.2.3.9000 package. `R` itself and all packages used are available 
from the Comprehensive `R` Archive Network (CRAN) at [https://CRAN.R-project.org/].

# REFERENCES {.unnumbered}

::: {#refs}
:::

\appendix

<!-- Store original definitions formatting -->
\let\oldclearpage\clearpage
\let\oldcleardoublepage\cleardoublepage

<!-- Disable page breaks -->
\let\clearpage\relax
\let\cleardoublepage\relax

# Fit Function

<!-- Restore original formatting -->
\let\clearpage\oldclearpage
\let\cleardoublepage\oldcleardoublepage

```{r}
#| prompt: true
.freq_itemsets_fit_arules <- function(x,
                                      min_support = NULL,
                                      mining_method = NULL) {

  if (is.null(min_support)) {
    rlang::abort(
      "Please specify `min_support` 
      to be able to fit specification.",
      call = call("fit")
    )
  }

  if (mining_method == "apriori") {
    res <- arules::apriori(data = x, parameter = list(
      support = min_support, 
      target = "frequent itemsets")
      )
  } else if (mining_method == "eclat") {
    res <- arules::eclat(
      data = x, 
      parameter = list(support = min_support)
      )
  } else {
    stop("Invalid method specified. Choose 'apriori' or 'eclat'.")
  }

  attr(res, "item_names") <- colnames(x)
  return(res)
}
```

# Cluster Functions

```{r}
#| prompt: true
extract_cluster_assignment.itemsets <- function(object, ...) {
  items <- attr(object, "item_names")
  itemsets <- arules::DATAFRAME(object)

  itemset_list <- lapply(strsplit(gsub(
    "[{}]", 
    "", 
    itemsets$items), 
    ","),
    stringr::str_trim)
  support <- itemsets$support
  clusters <- numeric(length(items))
  changed <- TRUE  # Flag to track convergence

  # Continue until no changes occur
  while (changed) {
    changed <- FALSE
    for (i in 1:length(items)) {
      current_item <- items[i]
      relevant_itemsets <- which(
        sapply(
          itemset_list,
          function(x) current_item %in% x
          )
        )

      if (length(relevant_itemsets) == 0) next

      # Find the best itemset (largest size, then highest support)
      best_itemset <- relevant_itemsets[
        which.max(
          sapply(itemset_list[relevant_itemsets], length) * 1000 + 
          support[relevant_itemsets]                                
          )
        ]
      best_itemset_size <- length(itemset_list[[best_itemset]])
      best_itemset_support <- support[best_itemset]

      # Current cluster info (if any)
      current_cluster <- clusters[i]
      current_cluster_size <- if (current_cluster != 0)
        length(itemset_list[[current_cluster]]) else 0
      current_cluster_support <- if (current_cluster != 0)
        support[current_cluster] else 0

      # Reassign if:
      # 1. No current cluster, OR
      # 2. New itemset is larger, OR
      # 3. Same size but higher support
      if (current_cluster == 0 ||
          best_itemset_size > current_cluster_size ||
          (best_itemset_size == current_cluster_size &&
          best_itemset_support > current_cluster_support)) {

        # Assign all items in the best itemset to its cluster
        new_cluster <- best_itemset
        items_in_best <- match(itemset_list[[best_itemset]], items)

        if (!all(clusters[items_in_best] == new_cluster)) {
          clusters[items_in_best] <- new_cluster
          changed <- TRUE  # Mark that a change occurred
        }
      }
    }
  }

  item_assignment_tibble_w_outliers(clusters, ...)
}

item_assignment_tibble_w_outliers <- function(clusters,
                                              ...,
                                              prefix = "Cluster_") {
  # Vector to store the resulting cluster names
  res <- character(length(clusters))

  # For items with cluster value 0, assign to "Cluster_0"
  res[clusters == 0] <- "Cluster_0"
  zero_count <- 0
  res <- sapply(res, function(x) {
    if (x == "Cluster_0") {
      zero_count <<- zero_count + 1
      paste0("Cluster_0_", zero_count)
    } else {
      x
    }
  })

  # For non-zero clusters, assign sequential cluster numbers
  non_zero_clusters <- clusters[clusters != 0]
  unique_non_zero_clusters <- unique(non_zero_clusters)

  # Map each unique non-zero cluster to a new cluster
  cluster_map <- setNames(paste0(
    prefix, 
    seq_along(unique_non_zero_clusters)), 
    unique_non_zero_clusters
    )

  # Assign the corresponding cluster names to the non-zero clusters
  res[clusters != 0] <- cluster_map[as.character(non_zero_clusters)]

  tibble::tibble(.cluster = factor(res))
}
```

# Predict Functions

```{r}
#| prompt: true
itemsets_predict_helper <- function(
    object, 
    new_data, 
    ..., 
    prefix = "Cluster_"
    ) {
  new_data <- as.data.frame(new_data)

  # Extract frequent itemsets and their supports
  items <- attr(object, "item_names")
  itemsets <- arules::DATAFRAME(object)
  frequent_itemsets <- lapply(strsplit(gsub(
    "[{}]", 
    "", 
    itemsets$items
    ), ","), 
    stringr::str_trim)
  supports <- itemsets$support

  # Calculate global support for each item (fallback)
  global_supports <- sapply(items, function(item) {
    containing <- sapply(frequent_itemsets, function(x) item %in% x)
    if (any(containing)) {
      sum(supports[containing]) / sum(containing)
    } else {
      0
    }
  })

  # Process each row of new_data
  result_list <- lapply(1:nrow(new_data), function(i) {
    row_data <- new_data[i, ]
    observed <- names(row_data)[row_data == 1]
    missing <- names(row_data)[is.na(row_data)]

    # Initialize prediction vector
    pred_values <- rep(NA, length(items))
    names(pred_values) <- items

    # Calculate probabilities for missing items
    for (item in missing) {
      # Itemsets containing current item and at least one observed
      relevant <- sapply(frequent_itemsets, function(x) {
        item %in% x && any(observed %in% x)
      })

      if (!any(relevant)) {
        pred_values[item] <- global_supports[item]
        next
      }

      # Calculate confidences
      confidences <- sapply(which(relevant), function(idx) {
        itemset <- frequent_itemsets[[idx]]
        itemset_without <- setdiff(itemset, item)

        # Find support of itemset without the current item
        if (length(itemset_without) == 0) return(NA)

        matches <- sapply(frequent_itemsets, 
                          function(x) identical(x, itemset_without))
        if (!any(matches)) return(NA)

        supports[idx] / supports[matches][1]
      })

      pred_values[item] <- mean(confidences, na.rm = TRUE)
      if (is.nan(pred_values[item])) {
        pred_values[item] <- global_supports[item]
      }
    }

    # Create result data frame
    data.frame(
      item = stringr::str_remove_all(items, "`"), # Remove backticks
      .obs_item = unlist(row_data),
      .pred_item = pred_values,
      row.names = NULL
    )
  })
}

.freq_itemsets_predict_raw_arules <- function(
    object, 
    new_data, 
    ..., 
    prefix = "Cluster_"
    ) {
  res <- itemsets_predict_helper(
    object, 
    new_data, 
    ..., 
    prefix = "Cluster_"
    )
  return(tibble::tibble(.pred_cluster = unname(res)))
}

.freq_itemsets_predict_arules <- function(
    object, 
    new_data, 
    ..., 
    prefix = "Cluster_"
    ) {
  res <- itemsets_predict_helper(
    object, 
    new_data, 
    ...,
    prefix = "Cluster_"
    )
  # Apply threshold to raw predictions
  lapply(res, function(df) {
    df$.pred_item <- ifelse(is.na(df$.obs_item),
                            ifelse(df$.pred_item >= 0.5, 1, 0),
                            NA)
    df
  })
}
```

```{r}
#| prompt: true
extract_predictions <- function(pred_output) {
  # Extract the list of data frames from the single column
  data_frames <- pred_output$.pred_cluster

  # Process each observation and combine results using reduce
  result_df <- data_frames %>%
    purrr::reduce(.f = ~ {
      # Process each observation (data frame)
      processed <- .y %>%
        dplyr::mutate(
          value = ifelse(
            !is.na(.obs_item), 
            .obs_item, 
            .pred_item)
          ) %>%
        dplyr::select(item, value) %>%
        tidyr::pivot_wider(names_from = item, values_from = value)

      # Combine the processed data frame with the results
      dplyr::bind_rows(.x, processed)
    }, .init = NULL)

  return(result_df)
}
```

# Metric Function

```{r}
#| prompt: true
augment_itemset_predict <- function(pred_output, truth_output) {
  # Extract all predictions (bind all .pred_cluster dataframes)
  preds_df <- dplyr::bind_rows(
    pred_output$.pred_cluster, 
    .id = "row_id"
    ) %>%
    dplyr::filter(!is.na(.pred_item)) %>%
    dplyr::mutate(item = stringr::str_remove_all(item, "`")) %>%
    dplyr::select(row_id, item, preds = .pred_item)

  # Pivot truth data to long format (to match predictions)
  truth_long <- truth_output %>%
    tibble::rownames_to_column("row_id") %>%
    tidyr::pivot_longer(
      cols = -row_id,
      names_to = "item",
      values_to = "truth_value"
    )

  # Join predictions with truth
  result <- preds_df %>%
    dplyr::inner_join(truth_long, by = c("row_id", "item"))

  # Return simplified output (preds vs truth)
  dplyr::select(result, item, row_id, preds, truth = truth_value)
}
  
random_na_with_truth <- function(df, na_prob = 0.3) {
  # Create a copy of the original dataframe to store truth values
  truth_df <- df

  # Create a mask of NAs (TRUE = becomes NA)
  na_mask <- matrix(
    sample(
      c(TRUE, FALSE),
      size = nrow(df) * ncol(df),
      replace = TRUE,
      prob = c(na_prob, 1 - na_prob)
    ),
    nrow = nrow(df)
  )

  # Apply the mask to create NA values
  na_df <- df
  na_df[na_mask] <- NA

  # Return both the NA-filled dataframe and the truth
  list(
    na_data = na_df,
    truth = truth_df
  )
}
```
