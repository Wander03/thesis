---
format:
  pdf:
    documentclass: report
    papersize: letter
    fontsize: 12pt
    mainfont: Times New Roman
    geometry:
      - left=1.5in
      - right=1in
      - top=1in
      - bottom=1in
    include-before-body:
      - title-page.tex
      - copyright-page.tex
      - committee-page.tex
      - abstract.tex
      - acknowledgments.tex
    toc: true
    lot: false
    lof: false
    number-sections: true
    include-in-header:
      text: |
        \AtBeginDocument{\pagenumbering{roman}}
        \usepackage{etoolbox}
        \apptocmd{\mainmatter}{%
          \pagenumbering{arabic}%
          \setcounter{page}{1}%
        }{}{}

        \usepackage{titlesec}
        \titleformat{\chapter}[display]
          {\normalfont\centering}
          {\chaptertitlename\ \thechapter}
          {0pt}
          {\MakeUppercase}
        \titleformat*{\section}{\normalfont\bfseries}
        \titleformat*{\subsection}{\normalfont\bfseries}
        \titleformat*{\subsubsection}{\normalfont\bfseries}
        \usepackage{setspace}
        \doublespacing
        \usepackage{etoolbox}
        \makeatletter
        \patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{plain}}{}{}
        \makeatother
bibliography: freq_itemsets_citations.bib
eval: false
---

```{r}
#| include: false
#| message: false

library(tidyverse)
library(tidyclust)
library(arules)

data(Groceries)
groceries <- as.data.frame(as(Groceries, "matrix")) %>%
  mutate(across(everything(), ~.*1))
```

\pagenumbering{arabic}
\setcounter{page}{1}

# Introduction

Since the early 1990s, frequent itemset mining (FIM) has been an important area of research in data mining. The work of Agrawal, Imieliński, and Swami [-@agrawal_mining_1993] introduced the Apriori algorithm, which established the foundation for identifying co-occurring items in transactional datasets. Shortly after, algorithms such as FP-growth’s FP-tree structure [@han_mining_2000] and Elcat’s vertical tid-list approach [@zaki_scalable_2000] significantly improved the performance and scalability of FIM techniques. These techniques have since been applied to a wide range of applications, including market basket analysis, web personalization [@mobasher_effective_2001], and associative classification [@liu_integrating_1998].

Although FIM has become more efficient, practical implementations remain confined to specialized software packages. The R programming environment offers itemset mining through `arules` [@hahsler_arules_2011], while Python users rely on `mlxtend` [@raschka_mlxtend_2018] for similar functionality. However, these implementations share the same limitation: they operate as stand alone itemset mining applications rather than as integrated components in a modern data science workflow. The absence of native integration with frameworks like `tidymodels` [@kuhn_tidymodels_2020] or `scikit-learn` [@buitinck_api_2013] leaves a gap, forcing analysts to create their own custom solutions.

Another limitation lies in the interpretation of FIM results. Frequent itemsets are commonly treated as either final outputs, or inputs for generating association rules. However, research by Cheng et al. [-@cheng_discriminative_2007] demonstrated that itemsets can also be used in classification tasks, while other studies have explored how itemsets can be used in clustering tasks [@mobasher_effective_2001; @wickramaratna_predicting_2009]. Although these approaches lack a standardized method, the use of FIM in these methods indicates a potential for integration with clustering workflows.

This thesis seeks to address these limitations by introducing FIM to `tidyclust` [@hvitfeldt_tidyclust_2022], a package designed for unsupervised learning algorithms within the `tidymodel` framework. By adapting the Apriori and Eclat algorithms to `tidyclust`, this work allows itemset mining to be used within unsupervised workflows. It also creates a standard methodology for predicting missing-items, and lays a foundation for future additions of mining and clustering techniques within `tidyclust`.

# Background

## Frequent Itemset Mining: Concepts and Algorithms

Frequent itemset mining (FIM) methods were developed to identify the elements of a transactional dataset that often occur together. The original use is in market basket research [@agrawal_mining_1993], where FIM techniques help discover items commonly bought together. For example, a pattern could be that milk and eggs are frequently bought together in the same transaction. This type of information can be used by the shop owner to place these items apart from each other, causing customers to walk through more of the store. Although this task may appear simple, the number of itemsets rapidly grows in wider datasets, resulting in the development of more efficient algorithms. One such development is the principle that if an itemset is frequent, then so are all of its subsets; a property known as the Apriori principle or downward closure property. This principle significantly reduces the search space by systematically pruning candidate itemsets that do not meet a minimum support threshold. The two methods implemented in `tidyclust`, Apriori and Eclat, both operate on this principle.

### Definitions

Some key terms used in frequent itemset mining have already been mentioned. In this section, we define and formalize these expressions.

Let $I = \{i_1, i_2, \dots, i_m\}$ be a set of items and $D = \{t_1, t_2, \dots, t_n\}$ a transactional dataset where each transaction $t_j \subseteq I$. A non-empty subset $X = \{i_1, i_2, ..., i_k\} \subseteq I$ is an itemset, or k-itemset where $k$ is the number of items.

The support of an itemset $X$ is the proportion of transactions containing $X$:

$$\text{support}(X) = \frac{|{t \in D \mid X ⊆ t}|}{|D|}$$

An itemset is considered frequent if its support is greater than or equal to a given minimum support threshold $\sigma$, where $0 \leq \sigma \leq 1$. The goal of FIM is to find the set of frequent itemsets corresponding to the users minimum support.

### The Apriori Algorithm

The Apriori algorithm [@agrawal_mining_1993] implements a breadth-first search to identify frequent itemsets, leveraging the downward closure property. The method operates as follows:

1.  Initialization ($k = 1$): Scan the transactional dataset $D$ to compute the support of all 1-itemsets $X$. Keep only those with support($X$) $\geq \sigma$, forming the set of frequent 1-itemsets $L_1$.

2.  Candidate Generation ($k > 1$): Generate candidate k-itemsets $C_k$ by joining pairs of frequent (k - 1)-itemsets from $L_{k-1}$ that share the first $k - 2$ items:
$$C_k = \{X \cup Y \mid X, Y \in L_{k - 1}, |X \cap Y| = k - 2\}$$

3.  Pruning: Eliminate any candidate $X \in C_k$ where (k - 1)-subset of $X$ is not in $L_{k - 1}$ (downward closure property)

4.  Support Counting: Scan the full dataset $D$ to compute the support($X$) for all $X \in C_k$.

5.  Iteration: Repeat steps 2-4 until no new frequent itemsets are found ($L_k = \emptyset$).

    $\bigcup_k L_k$ is the set of frequent itemsets.

### The Eclat Algorithm

The Equivalence Class Transformation (Eclat) algorithm [@zaki_scalable_2000] is an alternative to Apriori that uses a depth-first search strategy and vertical data representation. Instead of scanning the dataset repeatedly, Eclat represents transactions as tid-lists (transaction ID lists), which map each item or itemset to the IDs of transactions in which it appears. The method operates as follows:

1.  Vertical Representation: Transform the transaction dataset $D$ into a vertical format, where each item $x$ maintains its tid-list:
$$T(x) = {t \in D \mid x \in t}$$

Initialize the set of frequent 1-items:
$$L_1 = \{\{x\} \mid |T(x)| \geq \sigma \times |D|\}$$

2.  Depth-First Search: For each frequent itemset $X \in L_k$:

<!-- -->

a.  Candidate Generation: Extend $X$ with items $y > max(X)$ (lexicographic order) to form (k + 1)-itemset candidates $X' = X \cup \{y\}$.

b.  Tid-list Intersection: Compute the tid list of $X’$ with set intersection: $$T(X’) = \bigcap_{x \in X’} T(x)$$

c.  Support Verification: Keep $X’$ if: $$\frac{|T(X’)|}{|D|} \geq \sigma$$ Add $X’$ to $L_{k + 1}$

<!-- -->

3.  Termination: Repeat step 2 until no new frequent itemsets are found ($L_k = \emptyset$).

## tidyclust

The `tidyclust` package [@hvitfeldt_tidyclust_2022] extends the `tidymodels` [@kuhn_tidymodels_2020] framework to unsupervised tasks. These packages were built following the principles of the `tidyverse` [@wickham_welcome_2019], with the goal of establishing a consistent and reproducible workflow. Since the design of `tidyclust` is closely modeled off of `parsnip` [@kuhn_parsnip_2024], users are able to specify an unsupervised learning model, then fit and predict on the model using a standardized syntax.

### Model Specification

The `tidyclust` workflow mirrors established practices in unsupervised modeling. The first step involves model specification and selection. For example, a K-means model with three clusters would be specified as:

```{r}
kmeans_spec <- k_means(num_clusters = 3) %>%
  set_engine("stats")
```

The name of the function, `k_means`, is the name of the model. The parameter(s), here `num_clusters`, are the required and optional inputs for your model. In this example `num_clusters = 3` represents a K-means model with three clusters. The `set_engine()` function determines what package implementation of the model is being run, and in this example that is the `stats` package.

### Data Preprocessing

The next step is data preparation, and is taken care of by `recipes` [@kuhn_recipes_2024]. The `recipes` package support a range of data transformations, including: feature scaling (e.g., normalization, standardization), categorical data encoding (e.g., dummy variables), data cleaning (e.g., missing value removal/imputation), and dimensionality  reduction (e.g., PCA, feature filtering). The package follows a declarative syntax, improving both readability by separating the data preparation code from the modeling code and reproducibility by ensuring consistent transformations across training and testing data.

The following example uses the `palmerPenguins` dataset to create a recipe for K-means clustering:

```{r}
recipe <- recipe(
  ~ bill_length_mm + body_mass_g + sex, 
  data = penguins
  ) %>%
  step_naomit(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())
```

The recipe above defines what predictors to use in the model, then removes any rows with missing values within any of these predictors. It also normalizes the two numeric predictors and creates dummy variables for the categorical predictor. Since these transformations are applied within the recipe, they do not change the dataset itself. Additionally, the same recipe can be reused across different models built into the `tidyclust` or `tidymodels` framework. Once defined, the recipe can be combined with the model specification in a workflow.

### Modeling Workflow

The `workflows` package [@vaughan_workflows_2024] combines the model definition and feature engineering into a single object, improving reproducibility, reducing risk of human error, and standardizing the unsupervised learning pipeline. The modular design allows users to modify preprocessing steps without changing model specifications and reuse the same workflow elements across projects, ensuring consistency. An implementation using the previous examples would look as follows:

```{r}
kmeans_workflow <- workflow() %>%  
  add_recipe(recipe) %>%  
  add_model(kmeans_spec)
```

### Model Fitting

The final phase of the `tidyclust` workflow includes model fitting and prediction. The `fit()` function accepts a workflow and trains the model on the data, returning a `cluster_fit` object:

```{r}
kmeans_fit <- fit(kmeans_workflow, data = penguins)
```

This fitted object holds the trained model along with any metadata necessary for later operations.

### Model Prediction

The `predict()` function takes in the fitted object and new observations as inputs, outputting results in a standardized format:

```{r}
cluster_assignment <- predict(kmeans_fit, new_data = penguins_new)
```

The functionality of `predict()` depends on the type of fitted model, for example the K-means model from before would predict which of the three clusters each new observation in the input data belongs to.

### Hyperparameter Tuning and Model Evaluation

The `tidyclust` framework also encompasses model evaluation through through resampling and hyperparameter tuning capabilities. Grid search is implemented using the `tune` package [@kuhn_tune_2024], and can be paired with resampling methods such as cross-validation to evaluate a model. Using the K-means example above:

```{r}
penguins_cv <- vfold_cv(penguins, v = 5)

tune_spec <- k_means(num_clusters = tune()) %>%
  set_engine("stats")

tune_workflow <- workflow() %>%  
  add_recipe(recipe) %>%  
  add_model(tune_spec)

clust_num_grid <- grid_regular(num_clusters(3, 10),
  levels = 5
)

rf_grid_search <-
  tune_cluster(
    tune_workflow,
    resamples = penguins_cv,
    grid = clust_num_grid,
    metrics = cluster_metric_set(
      sse_within_total, 
      sse_total, 
      sse_ratio
      )
  )
```

First, cross-validation is set up specifying the number of folds (`v = 5`). In our model we change `num_clusters` from `3` to `tune()` and make a workflow. To establish the range of possible values for `num_clusters`, we use `grid_regular()`, then carry out the grid search in `tune_cluster()`. The metrics on each cross-validation split, for each possible choice in our grid, are specified within `tune_cluster()`.

The remaining sections of this thesis will be dedicated to the examination of the design and implementation choices made while implementing FIM in `tidyclust.` Section 3 covers each step of the workflow detailed above, Section 4 addresses current limitations, and Section 5 concludes with proposed enhancements.

# Methodology

## Model Specification

The model specification for frequent itemset mining follows the same structure as other models in `tidyclust`:

```{r}
fi_spec <- freq_itemsets(
  min_support = 0.05, 
  mining_method = "eclat"
  ) %>%
  set_engine("arules")
```

The only implemented engine is `arules`. There are two arguments: `min_support` is the user specified minimum support value for an itemset, taking a value between 0 and 1, and `mining_method` is an optional argument specifying the algorithm to use (`apriori` or `eclat`), the default being `eclat`. Eclat was selected as the default algorithm since it is faster in most practical cases [@singla_comprehensive_2023]. A default value for `min_support` is not supplied since this value greatly varies depending on the characteristics of the data.

Recipes and workflows are not impacted by any additions that I have made to `tidyclust`. These tools are the same and work with `freq_itemsets` without error.

## Cluster Assignment Methodology

While fitting a `freq_itemsets` model is the same, multiple decisions had to be made to create the desired output:

```{r}
fi_fit <- fi_spec %>%
  fit(~ .,
    data = groceries
  )
```

A strength of the `tidyclust` framework is that the output of each function is standardized, formatted the same way. The main product of a fitted model are the cluster assignments, which are stored in a `tibble` with the sole column `.cluster`.

```{r}
fi_fit %>% 
  extract_cluster_assignment()
```

However, unlike other unsupervised learning algorithms (e.g., K-means) that explicitly partition observations into distinct groups, FIM produces a set of co-occuring items without inherent cluster labels. Additionally, in FIM the items (columns) are of interest, while in other unsupervised learning algorithms the observations (rows) are of interest. The following implementation introduces a clustering assignment strategy that assigns items to clusters based on itemset membership and support values:

For each items in the dataset:

1. Relevant Itemset Identification: Finds all itemsets containing the item.

2. Best Itemset Selection: Prioritizes itemsets by:

    + Size: Larger itemsets over smaller ones (encouraging broader clusters).

    + Support: Higher-support itemsets break ties (favoring more frequent patterns).

3. Cluster Propagation: Assigns all items in the selected itemsets to the same cluster. 

If an item has already been assigned a Cluster, then the best itemset is compared to the best itemset used for this Cluster assignment. The same itemset prioritization is applied, and if the new best itemset is superior then all relevant items are assigned to a new Cluster.

Items that appear in no frequent itemsets are labeled as outliers (Cluster_0_X) while items within frequent itemsets are assigned sequential cluster IDs (Cluster_1, Cluster_2, etc.).

Cheng et al. [-@cheng_discriminative_2007] analyzes the relationship between pattern frequency and discriminative power, demonstrating that frequent patterns with higher support and larger sizes exhibit greater predictive utility in classification tasks. These findings are used as the reasoning for prioritizing larger itemsets with higher supports. 

## Prediction Methods

Similar to the fit method, while the syntax for predicting using a fitted `freq_itemsets` model is the same, the input and methodology of the predict function differs from other `tidyclust` models:

```{r}
fi_pred <- fi_fit %>%
  predict(new_data = new_groceries)
```

When deciding what it means to predict using frequent itemsets, my goal was to create a method that has practical use in a real world setting. This lead to implementing a recommendation system, where the new data will have partial information about a transaction and predicts which other items are likely to be in the transaction. For example, suppose a customer in a grocery store items `A` and `B` in their cart. This system would predict which other items they are likely to add based on historical purchasing patterns where `A` and `B` appeared together.

The implementation of the recommendation system was inspired by traditional association rule mining such as CARs [@liu_integrating_1998] and usage-based recommendations [@mobasher_effective_2001]. The method predicts item-level probabilities using the confidence from frequent itemsets, retains interpretability from a rule-based reasoning approach, and handles sparse data through a global support fallback. Specifically, for each transaction in the new data:

1. Missing Item Identification: Finds all missing items in the transaction.

For each missing item:

1. Relevant Itemset Identification: Finds all itemsets containing the item.

2. Context Filtering: Retain itemsets that include at least one observed item from the transaction.

3. Confidence Aggregation: For each retained itemset, compute the confidence of the association where
$$\text{confidence} = \frac{\text{support(itemset)}}{\text{support(observed items in itemset)}}$$

4. Prediction Generation: Average the confidences across all retained itemsets.

5. Fallback Handling: If there are no relevant or retained itemsets, use the items global support (frequency in training data) as the prediction. If the item does not appear in any frequent itemsets, then this global support will be 0.

Aside from the `new_data` argument, `predict()` has the optional `type` argument which takes either `'raw'` or `'cluster'`. The `'raw'` option returns the raw prediction probabilities, while the default option `'cluster'` applies a threshold (0.5) to convert probabilities to binary predictions. Both options return the predictions as a `tibble` with the sole column `.pred_cluster`. 

While this `tibble` format is standardized across `tidyclust` predict outputs, it does not provide the information necessary for FIM predict output since FIM prediction on the items (columns) and not the observations (rows). To keep this standardized formatting while providing the necessary information, each row of the output `tibble` represents a transaction from the new data and holds a dataframe rather than a string or integer. This dataframe has three columns: `item`, `.obs_item`, and `.pred_item`. The `item` column represents each item (column) from the transaction, and `.obs_item` is `1` or `0` for any observed items and `NA` for any missing items. The `.pred_item` column is the opposite of the `.obs_item` column, where any observed items are `NA` and any missing items are either the raw or binary predicted values. This structure allows the user to easily identify what values are observed or predicted in each transaction while still containing all the information necessary to recreate the original transaction dataset. To assist the user with reconstruction, the function `extract_predictions` takes the `tibble` output and reformates it to a single data frame.

```{r}
fi_pred %>%
  extract_predictions
```


## Hyperparameter Tuning and Performance Metrics

# Limitations

## Analysis of Implementation Limitations

## Future Directions

# References

::: {#refs}
:::
